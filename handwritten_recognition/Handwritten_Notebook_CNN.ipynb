{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Handwritten numbers Recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extraer datos. Las imágenes son de 28*28 píxeles, por lo tanto el csv tiene 784 columnas más 1 indicando la salida"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract():\n",
    "    df_train = pd.read_csv('data/mnist_train.csv', delimiter = ',').to_numpy()\n",
    "    df_test = pd.read_csv('data/mnist_test.csv', delimiter = ',').to_numpy()\n",
    "    df = pd.DataFrame(np.concatenate((df_train,df_test)))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transformar output a array de 10 y separar de las inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform(df): \n",
    "    y_raw = df.iloc[:, 0].to_numpy()\n",
    "    x = df.drop(df.columns[0], axis='columns').to_numpy()\n",
    "    y = []\n",
    "    for i in range(0,y_raw.shape[0]):\n",
    "        row=np.zeros(10)\n",
    "        row[y_raw[i]]=1\n",
    "        y.append(row)\n",
    "    y=np.array(y)\n",
    "    y.reshape(y.shape[0],10)\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creamos los dataset de entrenamiento y test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = extract()\n",
    "x_train,y_train = transform(df.iloc[0:60000])\n",
    "#x_val,y_val = transform(df.iloc[50000:60000])\n",
    "x_test,y_test = transform(df.iloc[60000:70000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = x_train.reshape(60000,28,28,1)\n",
    "x_test = x_test.reshape(9998,28,28,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creación de la Red Neuronal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En la red neuronal vamoas a implementar una arquitectura de (784,512,256,128,10), con funciones de activación ReLu en todas las capas menos en la última, en la cual utilizaremos la funcion softmax (https://www.quora.com/Why-is-it-better-to-use-Softmax-function-than-sigmoid-function).<br> Para cada capa se utilizará la funcion Dense, la cual tiene los siguientes parámetros por defecto:\n",
    "- activation=None,\n",
    "- use_bias=True,\n",
    "- kernel_initializer='glorot_uniform',\n",
    "- bias_initializer='zeros',\n",
    "- kernel_regularizer=None,\n",
    "- bias_regularizer=None,\n",
    "- activity_regularizer=None,\n",
    "- kernel_constraint=None,\n",
    "- bias_constraint=None\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Conv2D, Flatten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "#add model layers\n",
    "model.add(Conv2D(64, kernel_size=3, activation='relu', input_shape=(28,28,1)))\n",
    "model.add(Conv2D(32, kernel_size=3, activation='relu'))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(10, activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a crear y entrenar el modelo, 50 epochs serán suficientes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 9998 samples\n",
      "Epoch 1/5\n",
      "60000/60000 [==============================] - 80s 1ms/step - loss: 0.2252 - accuracy: 0.9515 - val_loss: 0.0898 - val_accuracy: 0.9717\n",
      "Epoch 2/5\n",
      "60000/60000 [==============================] - 81s 1ms/step - loss: 0.0680 - accuracy: 0.9793 - val_loss: 0.1020 - val_accuracy: 0.9729\n",
      "Epoch 3/5\n",
      "60000/60000 [==============================] - 81s 1ms/step - loss: 0.0479 - accuracy: 0.9850 - val_loss: 0.0888 - val_accuracy: 0.9733\n",
      "Epoch 4/5\n",
      "60000/60000 [==============================] - 82s 1ms/step - loss: 0.0361 - accuracy: 0.9885 - val_loss: 0.1055 - val_accuracy: 0.9734\n",
      "Epoch 5/5\n",
      "60000/60000 [==============================] - 83s 1ms/step - loss: 0.0273 - accuracy: 0.9912 - val_loss: 0.1120 - val_accuracy: 0.9776\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x26617815f08>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])#learning_rate=0.001\n",
    "model.fit(x_train, y_train,validation_data=(x_test, y_test), epochs=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluamos el modelo con el acierto de los datos de prueba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9998/9998 [==============================] - 2s 213us/step\n",
      "Test Acc:  0.9775955080986023\n"
     ]
    }
   ],
   "source": [
    "tes_loss, test_acc = model.evaluate(x_test, y_test)\n",
    "print('Test Acc: ', test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regularización"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras import regularizers\n",
    "modelR = keras.Sequential([\n",
    "    keras.layers.Dense(784, input_dim=784, kernel_regularizer=regularizers.l2(0.01), bias_regularizer=regularizers.l2(0.01)),\n",
    "    keras.layers.Dense(512, activation=\"relu\", kernel_regularizer=regularizers.l2(0.01), bias_regularizer=regularizers.l2(0.01)),\n",
    "    keras.layers.Dense(256, activation=\"relu\", kernel_regularizer=regularizers.l2(0.01), bias_regularizer=regularizers.l2(0.01)),\n",
    "    keras.layers.Dense(128, activation=\"relu\", kernel_regularizer=regularizers.l2(0.01), bias_regularizer=regularizers.l2(0.01)),\n",
    "    keras.layers.Dense(10, activation=\"softmax\", kernel_regularizer=regularizers.l2(0.01), bias_regularizer=regularizers.l2(0.01))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples\n",
      "Epoch 1/50\n",
      "60000/60000 [==============================] - 4s 70us/sample - loss: 17.9050 - accuracy: 0.8753\n",
      "Epoch 2/50\n",
      "60000/60000 [==============================] - 4s 62us/sample - loss: 10.8157 - accuracy: 0.9478\n",
      "Epoch 3/50\n",
      "60000/60000 [==============================] - 4s 63us/sample - loss: 7.6734 - accuracy: 0.9663\n",
      "Epoch 4/50\n",
      "60000/60000 [==============================] - 4s 63us/sample - loss: 5.4694 - accuracy: 0.9738\n",
      "Epoch 5/50\n",
      "60000/60000 [==============================] - 5s 79us/sample - loss: 3.9047 - accuracy: 0.9774\n",
      "Epoch 6/50\n",
      "60000/60000 [==============================] - ETA: 0s - loss: 2.8039 - accuracy: 0.97 - 5s 81us/sample - loss: 2.7996 - accuracy: 0.9777\n",
      "Epoch 7/50\n",
      "60000/60000 [==============================] - 5s 81us/sample - loss: 2.0298 - accuracy: 0.9759\n",
      "Epoch 8/50\n",
      "60000/60000 [==============================] - 5s 80us/sample - loss: 1.4890 - accuracy: 0.9756\n",
      "Epoch 9/50\n",
      "60000/60000 [==============================] - 5s 80us/sample - loss: 1.1215 - accuracy: 0.9732\n",
      "Epoch 10/50\n",
      "60000/60000 [==============================] - 5s 88us/sample - loss: 0.8735 - accuracy: 0.9709\n",
      "Epoch 11/50\n",
      "60000/60000 [==============================] - 6s 98us/sample - loss: 0.7039 - accuracy: 0.9696\n",
      "Epoch 12/50\n",
      "60000/60000 [==============================] - 7s 121us/sample - loss: 0.5830 - accuracy: 0.9698\n",
      "Epoch 13/50\n",
      "60000/60000 [==============================] - 6s 93us/sample - loss: 0.5107 - accuracy: 0.9661\n",
      "Epoch 14/50\n",
      "60000/60000 [==============================] - 5s 90us/sample - loss: 0.4591 - accuracy: 0.9645\n",
      "Epoch 15/50\n",
      "60000/60000 [==============================] - 5s 88us/sample - loss: 0.4196 - accuracy: 0.9637\n",
      "Epoch 16/50\n",
      "60000/60000 [==============================] - 5s 85us/sample - loss: 0.3968 - accuracy: 0.9615\n",
      "Epoch 17/50\n",
      "60000/60000 [==============================] - 5s 78us/sample - loss: 0.3644 - accuracy: 0.9642\n",
      "Epoch 18/50\n",
      "60000/60000 [==============================] - 5s 79us/sample - loss: 0.3602 - accuracy: 0.9599\n",
      "Epoch 19/50\n",
      "60000/60000 [==============================] - 5s 80us/sample - loss: 0.3403 - accuracy: 0.9620\n",
      "Epoch 20/50\n",
      "60000/60000 [==============================] - 5s 90us/sample - loss: 0.3235 - accuracy: 0.9635\n",
      "Epoch 21/50\n",
      "60000/60000 [==============================] - 5s 89us/sample - loss: 0.3232 - accuracy: 0.9621\n",
      "Epoch 22/50\n",
      "60000/60000 [==============================] - 5s 83us/sample - loss: 0.3097 - accuracy: 0.9642\n",
      "Epoch 23/50\n",
      "60000/60000 [==============================] - 5s 84us/sample - loss: 0.3122 - accuracy: 0.9634\n",
      "Epoch 24/50\n",
      "60000/60000 [==============================] - 5s 82us/sample - loss: 0.3065 - accuracy: 0.9642\n",
      "Epoch 25/50\n",
      "60000/60000 [==============================] - 5s 80us/sample - loss: 0.3164 - accuracy: 0.9627\n",
      "Epoch 26/50\n",
      "60000/60000 [==============================] - 5s 80us/sample - loss: 0.3010 - accuracy: 0.9653\n",
      "Epoch 27/50\n",
      "60000/60000 [==============================] - 5s 81us/sample - loss: 0.3069 - accuracy: 0.9635\n",
      "Epoch 28/50\n",
      "60000/60000 [==============================] - 5s 81us/sample - loss: 0.2983 - accuracy: 0.9654\n",
      "Epoch 29/50\n",
      "60000/60000 [==============================] - 5s 81us/sample - loss: 0.3059 - accuracy: 0.9644\n",
      "Epoch 30/50\n",
      "60000/60000 [==============================] - 5s 80us/sample - loss: 0.2958 - accuracy: 0.9667\n",
      "Epoch 31/50\n",
      "60000/60000 [==============================] - 5s 80us/sample - loss: 0.3125 - accuracy: 0.9636\n",
      "Epoch 32/50\n",
      "60000/60000 [==============================] - 5s 80us/sample - loss: 0.3046 - accuracy: 0.9646\n",
      "Epoch 33/50\n",
      "60000/60000 [==============================] - 5s 80us/sample - loss: 0.3102 - accuracy: 0.9645\n",
      "Epoch 34/50\n",
      "60000/60000 [==============================] - 5s 80us/sample - loss: 0.3071 - accuracy: 0.9645\n",
      "Epoch 35/50\n",
      "60000/60000 [==============================] - 5s 80us/sample - loss: 0.2871 - accuracy: 0.9683\n",
      "Epoch 36/50\n",
      "60000/60000 [==============================] - 5s 80us/sample - loss: 0.3117 - accuracy: 0.9647\n",
      "Epoch 37/50\n",
      "60000/60000 [==============================] - 5s 80us/sample - loss: 0.3140 - accuracy: 0.9646\n",
      "Epoch 38/50\n",
      "60000/60000 [==============================] - 5s 80us/sample - loss: 0.3104 - accuracy: 0.9648\n",
      "Epoch 39/50\n",
      "60000/60000 [==============================] - 5s 82us/sample - loss: 0.3020 - accuracy: 0.9657\n",
      "Epoch 40/50\n",
      "60000/60000 [==============================] - 5s 83us/sample - loss: 0.3145 - accuracy: 0.9658\n",
      "Epoch 41/50\n",
      "60000/60000 [==============================] - 5s 80us/sample - loss: 0.2815 - accuracy: 0.9693\n",
      "Epoch 42/50\n",
      "60000/60000 [==============================] - 5s 80us/sample - loss: 1.4696 - accuracy: 0.7857\n",
      "Epoch 43/50\n",
      "60000/60000 [==============================] - 5s 80us/sample - loss: 0.6708 - accuracy: 0.9278\n",
      "Epoch 44/50\n",
      "60000/60000 [==============================] - 5s 81us/sample - loss: 0.4523 - accuracy: 0.9470\n",
      "Epoch 45/50\n",
      "60000/60000 [==============================] - 5s 81us/sample - loss: 0.3707 - accuracy: 0.9572\n",
      "Epoch 46/50\n",
      "60000/60000 [==============================] - 5s 81us/sample - loss: 0.3352 - accuracy: 0.9607\n",
      "Epoch 47/50\n",
      "60000/60000 [==============================] - 5s 81us/sample - loss: 0.3142 - accuracy: 0.9644\n",
      "Epoch 48/50\n",
      "60000/60000 [==============================] - 5s 80us/sample - loss: 0.3057 - accuracy: 0.9641\n",
      "Epoch 49/50\n",
      "60000/60000 [==============================] - 5s 80us/sample - loss: 0.3010 - accuracy: 0.9656\n",
      "Epoch 50/50\n",
      "60000/60000 [==============================] - 5s 80us/sample - loss: 0.2985 - accuracy: 0.9664\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1bf83c28708>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modelR.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])#learning_rate=0.001\n",
    "modelR.fit(x_train, y_train, epochs = 50, batch_size=200, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9998/9998 [==============================] - 2s 244us/sample - loss: 0.2920 - accuracy: 0.9649\n",
      "Test Acc:  0.964893\n"
     ]
    }
   ],
   "source": [
    "tes_loss, test_acc = modelR.evaluate(x_test, y_test)\n",
    "print('Test Acc: ', test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
